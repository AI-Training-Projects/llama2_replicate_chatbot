"""
streamlit_app_v4.py

This module sets up a Streamlit application for a chatbot using the Llama 2 model from Meta.
It loads environment variables, sets up logging, retrieves API keys, and configures the Streamlit app.
The app allows users to interact with the chatbot and generate responses using the Replicate API.

## TODO: Clarify the selected_model and how this changes the chatbot's behavior and which LLM model is actually on Replicate backend.

Generated by Rich Lysakowski
"""

import streamlit as st
import replicate
import os
import logging
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Set up logging
logging.basicConfig(filename="app.log", level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Retrieve API keys from environment variables
replicate_api = os.getenv("REPLICATE_API_TOKEN")
langchain_api = os.getenv("LANGCHAIN_API_KEY")
llama_index_api = os.getenv("LLAMA_INDEX_API_KEY")
openai_api = os.getenv("OPENAI_API_KEY")
assemblyai_api = os.getenv("ASSEMBLYAI_API_KEY")
huggingface_api = os.getenv("HUGGINGFACE_API_KEY")

# Check if API keys are loaded
if not replicate_api:
    logging.error("Replicate API token not found in environment variables.")
    st.error("Replicate API token not found. Please check your .env file.")
else:
    logging.info("Replicate API token loaded successfully.")

# App title
st.set_page_config(page_title="ü¶ôüí¨ Llama 2 Chatbot")

# Replicate Credentials
with st.sidebar:
    st.title("ü¶ôüí¨ Llama 2 Chatbot")
    st.write("This chatbot is created using the open-source Llama 2 LLM model from Meta.")
    if replicate_api:
        st.success("API key already provided!", icon="‚úÖ")
    else:
        replicate_api = st.text_input("Enter Replicate API token:", type="password")
        if not (replicate_api.startswith("r8_") and len(replicate_api) == 40):
            st.warning("Please enter your credentials!", icon="‚ö†Ô∏è")
        else:
            st.success("Proceed to entering your prompt message!", icon="üëâ")
    os.environ["REPLICATE_API_TOKEN"] = replicate_api

    st.subheader("Models and parameters")
    selected_model = st.selectbox("Choose a Llama2 model", ["Llama2-13B-chat", "Llama2-7B"], key="selected_model")
    temperature = st.slider("temperature", min_value=0.01, max_value=1.0, value=0.1, step=0.01)
    top_p = st.slider("top_p", min_value=0.01, max_value=1.0, value=0.9, step=0.01)
    max_length = st.slider("max_length", min_value=20, max_value=800, value=50, step=5)
    st.markdown("üìñ Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!")

# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    """
    Clears the chat history by resetting the messages in the session state.
    """
    st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]
st.sidebar.button("Clear Chat History", on_click=clear_chat_history)

def generate_llama2_response(prompt_input):
    """
    Generates a response from the Llama 2 model using the Replicate API.

    Args:
        prompt_input (str): The user-provided prompt.

    Returns:
        str: The generated response or an error message.
    """
    try:
        string_dialogue = "You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'."
        for dict_message in st.session_state.messages:
            string_dialogue += f"{dict_message['role']}: {dict_message['content']} "
        output = replicate.run(
            "meta/llama-2-13b-chat",
            input={"prompt": f"{string_dialogue} {prompt_input} Assistant: ", "temperature": temperature, "top_p": top_p, "max_length": max_length, "repetition_penalty": 1}
        )
        return output
    except Exception as e:
        logging.error(f"Error generating LLaMA2 response: {e}")
        st.error("An error occurred while generating the response. Please try again.")
        return "Error generating response."

# User-provided prompt
if prompt := st.chat_input(disabled=not replicate_api):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        response = generate_llama2_response(st.session_state.messages[-1]["content"])
        st.write(response)
        st.session_state.messages.append({"role": "assistant", "content": response})

# Generated by Rich Lysakowski